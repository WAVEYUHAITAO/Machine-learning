{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.张量的数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入需要的库\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取张量的数据类型\n",
    "torch.tensor([1.2,3.4]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#张量的默认数据类型设置为其他类型\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.tensor([1.2,3.4]).dtype\n",
    "#注意：set_default_dtype()只支持设置浮点类型数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.dtype: torch.float64\n",
      "a.long()方法: torch.int64\n",
      "a.int()方法: torch.int32\n",
      "a.float()方法: torch.float32\n"
     ]
    }
   ],
   "source": [
    "##将张量数据类型转化为整形\n",
    "a=torch.tensor([1.2,3.4])\n",
    "print(\"a.dtype:\",a.dtype)\n",
    "print(\"a.long()方法:\",a.long().dtype)\n",
    "print(\"a.int()方法:\",a.int().dtype)\n",
    "print(\"a.float()方法:\",a.float().dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#恢复torch默认的数据类型\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "torch.tensor([1.2,3.4]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取默认的数据类型\n",
    "torch.get_default_dtype()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.生成张量"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1基本方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [2., 2.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "A = torch.tensor([[1.0,1.0],[2,2]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 获取张量的形状\n",
    "A.shape\n",
    "A.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 获取张量的形状\n",
    "A.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 计算张量中所含元素的个数\n",
    "A.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 指定张量的数据类型和是否计算梯度\n",
    "B = torch.tensor((1,2,3),dtype=torch.float32,requires_grad=True)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14., grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 42.,  84., 126.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 因为张量B是可以计算梯度的，所以可以计算sum(B^2)的梯度\n",
    "y = B.pow(2).sum()\n",
    "print(y)\n",
    "y.backward()\n",
    "B.grad #每一次的梯度会加上之前得到的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Only Tensors of floating point and complex dtype can require gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\HT\\AppData\\Local\\Temp\\ipykernel_956\\3532308131.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## 注意只有浮点类型的张量才允许计算梯度\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Only Tensors of floating point and complex dtype can require gradients"
     ]
    }
   ],
   "source": [
    "## 注意只有浮点类型的张量才允许计算梯度\n",
    "B = torch.tensor((1,2,3),dtype=torch.int32,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 利用torch.Tensor()获得张量\n",
    "C = torch.Tensor([1,2,3,4])\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.5764e-07, 1.2801e-11, 2.1029e+20],\n",
       "        [5.4885e-05, 8.4936e+20, 3.1986e+21]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 创建具有特定大小的张量\n",
    "D = torch.Tensor(2,3)\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 创建与另一个张量大小和类型相同的张量\n",
    "torch.ones_like(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros_like(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8906, 0.1025, 0.3625],\n",
       "        [0.4898, 0.1541, 0.6386]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand_like(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D.dtype: torch.float32\n",
      "E.dtype: torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 创建一个类型相似但尺寸不同的张量\n",
    "E = [[1,2],[3,4]]\n",
    "#print(\"E.dtype:\",E.dtype)\n",
    "#E 目前是一个list\n",
    "E = D.new_tensor(E)\n",
    "print(\"D.dtype:\",D.dtype)\n",
    "print(\"E.dtype:\",E.dtype)\n",
    "#E 编程一个tensor\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.new_full((3,3),fill_value = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.new_zeros((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.new_empty((3,3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2利用numpy数组生成张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "F = np.ones((3,3))\n",
    "print(F)\n",
    "##使用torch.as_tensor()函数\n",
    "Ftensor = torch.as_tensor(F)\n",
    "Ftensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 使用torch.from_numpy()函数\n",
    "Ftensor = torch.from_numpy(F)\n",
    "Ftensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 使用张量的.numpy()将张量转化为numpy数组\n",
    "Ftensor.numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3设置随机数种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x29cb4e539d0>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 设置随机数种子\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1115)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 通过指定均值和标准差生成随机数\n",
    "torch.manual_seed(123)\n",
    "A = torch.normal(mean = 0.0,std = torch.tensor(1.0))\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1115)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 通过指定均值和标准差生成随机数\n",
    "import torch\n",
    "torch.manual_seed(123)\n",
    "A = torch.normal(mean = torch.tensor(0.0),std = 1.0)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1115,  0.2407, -1.1089, -0.9617])\n",
      "torch.float32\n",
      "tensor([-1.1969,  0.4185, -2.9171, -3.0202,  1.6195])\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "## 通过指定均值和标准差生成随机数\n",
    "torch.manual_seed(123)\n",
    "A = torch.normal(mean = 0.0, std=torch.arange(1,5.0))#不包含END \n",
    "B = torch.normal(mean = 0.0, std=torch.range(1,5.0))#包含END\n",
    "print(A)\n",
    "print(A.dtype)\n",
    "print(B)\n",
    "print(B.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2961, 0.5166, 0.2517, 0.6886],\n",
       "        [0.0740, 0.8665, 0.1366, 0.1025],\n",
       "        [0.1841, 0.7264, 0.3153, 0.6871]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 在区间（0,1）上生成服从均匀分布的张量\n",
    "torch.manual_seed(123)\n",
    "B = torch.rand(3,4)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2961, 0.5166, 0.2517],\n",
       "        [0.6886, 0.0740, 0.8665]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成和其他张量尺寸相同的随机数张量\n",
    "torch.manual_seed(123)\n",
    "C = torch.ones(2,3)\n",
    "D = torch.rand_like(C)\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9447,  0.6217, -1.3501],\n",
      "        [-0.1881, -2.3891, -0.4759],\n",
      "        [ 1.7603,  0.6547,  0.5490]])\n",
      "tensor([[ 0.3671,  0.1219,  0.6466],\n",
      "        [-1.4168,  0.8429, -0.6307]])\n"
     ]
    }
   ],
   "source": [
    "## 生成服从标准正态分布的随机数\n",
    "print(torch.randn(3,3))\n",
    "print(torch.randn_like(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 8, 1, 3, 7, 4, 9, 5, 6])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 将0~10(不包括10)之间的整数随机排序\n",
    "torch.manual_seed(123)\n",
    "torch.randperm(10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4其他生成张量的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 4, 6, 8])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 使用torch.arange()生成张量\n",
    "torch.arange(start=0, end=10, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  3.2500,  5.5000,  7.7500, 10.0000])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 在范围内生成固定数量的等间隔张量\n",
    "torch.linspace(start=1,end=10,steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2589,  2.1135,  3.5481,  5.9566, 10.0000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 生成以对数间隔的点\n",
    "torch.logspace(start=0.1,end=1.0,steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2589,  2.1135,  3.5481,  5.9566, 10.0000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**torch.linspace(start=0.1,end=1.0,steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.3250, 0.5500, 0.7750, 1.0000])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(start=0.1,end=1.0,steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.zeros(3,3)\n",
    "#torch.ones(3,3)\n",
    "torch.eye(3)\n",
    "#torch.empty(3,3)\n",
    "#torch.full((3,3),fill_value=0.25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.张量的操作"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1改变张量的尺寸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 使用tensor.reshape()函数设置张量的尺寸\n",
    "A = torch.arange(12.0).reshape(3,4)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
       "        [ 6.,  7.,  8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 使用torch.reshape()\n",
    "torch.reshape(input=A, shape=(2,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
       "        [ 6.,  7.,  8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 使用resize_方法\n",
    "A.resize_(2,6)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 5.],\n",
       "        [6., 7., 8.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 使用\n",
    "B = torch.arange(10.0,19.0).reshape(3,3)\n",
    "B\n",
    "A.resize_as_(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 11., 12.],\n",
       "        [13., 14., 15.],\n",
       "        [16., 17., 18.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
       "         [ 6.,  7.,  8.,  9., 10., 11.]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## torch.unsqueeze()返回在指定维度插入尺寸为1的新张量\n",
    "A=torch.arange(12.0).reshape(2,6)\n",
    "B=torch.unsqueeze(A,dim=0)\n",
    "print(B.shape)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C.shape: torch.Size([1, 2, 6, 1])\n",
      "D.shape: torch.Size([2, 6])\n",
      "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.,  9., 10., 11.]])\n",
      "E.shape: torch.Size([2, 6, 1])\n",
      "tensor([[[ 0.],\n",
      "         [ 1.],\n",
      "         [ 2.],\n",
      "         [ 3.],\n",
      "         [ 4.],\n",
      "         [ 5.]],\n",
      "\n",
      "        [[ 6.],\n",
      "         [ 7.],\n",
      "         [ 8.],\n",
      "         [ 9.],\n",
      "         [10.],\n",
      "         [11.]]])\n"
     ]
    }
   ],
   "source": [
    "## torch.squeeze()函数移除所有维度为1的维度\n",
    "C= B.unsqueeze(dim=3)\n",
    "print(\"C.shape:\",C.shape)\n",
    "#print(C)\n",
    "D= torch.squeeze(C)\n",
    "print(\"D.shape:\",D.shape)\n",
    "print(D)\n",
    "## 移除指定维度为1的维度\n",
    "E = torch.squeeze(C,dim=0)\n",
    "print(\"E.shape:\",E.shape)\n",
    "print(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [0, 1, 2],\n",
       "        [0, 1, 2]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 使用.expand()方法拓展张量\n",
    "\n",
    "A=torch.arange(3)\n",
    "A\n",
    "B=A.expand(3,-1)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [0, 1, 2]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 使用expand_as()方法拓展张量\n",
    "C= torch.arange(6).reshape(2,3)\n",
    "B= A.expand_as(C)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 1, 2, 0, 1, 2],\n",
      "         [0, 1, 2, 0, 1, 2],\n",
      "         [0, 1, 2, 0, 1, 2],\n",
      "         [0, 1, 2, 0, 1, 2]]])\n",
      "torch.Size([1, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "## 使用.repeat()方法拓展张量\n",
    "## B的shape是(2,3)，首先扩展橙(1,2,3)再按照repeat重复变为(1,4,6)\n",
    "D= B.repeat(1,2,2)\n",
    "print(D)\n",
    "print(D.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2获取张量中的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 利用切片和索引获取张量中的元素\n",
    "A = torch.arange(12).reshape(1,3,4)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3],\n",
       "        [4, 5, 6, 7]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 获取第0维度下的矩阵前两行元素\n",
    "A[0,0:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8,  9, 10])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 获取第0维度下的矩阵，最后一行-4~-1列\n",
    "A[0,-1,-4:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0, -1, -2, -3],\n",
       "         [-4, -5,  6,  7],\n",
       "         [ 8,  9, 10, 11]]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 根据条件筛选\n",
    "B=-A\n",
    "torch.where(A>5,A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 获取A中某个元素\n",
    "A[0,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  0,  0,  0],\n",
       "         [ 4,  5,  0,  0],\n",
       "         [ 8,  9, 10,  0]]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 获取矩阵张量的下三角部分\n",
    "torch.tril(A,diagonal=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  0,  0],\n",
       "         [ 4,  5,  6,  0],\n",
       "         [ 8,  9, 10, 11]]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## diagonal参数控制对角线从哪个元素开始\n",
    "torch.tril(A,diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 0,  5,  6,  7],\n",
       "         [ 0,  0, 10, 11]]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 获取矩阵张量的上三角部分\n",
    "torch.triu(A,diagonal=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 0,  0,  6,  7],\n",
       "         [ 0,  0,  0, 11]]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(A,diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([ 0,  5, 10])\n",
      "tensor([ 1,  6, 11])\n"
     ]
    }
   ],
   "source": [
    "## 获取矩阵张量的上三角部分，input,需要是一个二维的张量\n",
    "C = A.reshape(3,4)\n",
    "print(C)\n",
    "print(torch.diag(C,diagonal=0))\n",
    "print(torch.diag(C,diagonal=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 3]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 提供对角线元素生成矩阵张量\n",
    "torch.diag(torch.tensor([1,2,3]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3张量的拼接和拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.],\n",
       "        [ 3.,  4.,  5.],\n",
       "        [ 0.,  2.,  4.],\n",
       "        [ 6.,  8., 10.]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 在给定维度中连接给定的张量序列\n",
    "A = torch.arange(6.0).reshape(2,3)\n",
    "B = torch.linspace(0,10,6).reshape(2,3)\n",
    "## 在0维度连接张量\n",
    "C = torch.cat((A,B),dim=0)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  0.,  2.,  4.],\n",
       "        [ 3.,  4.,  5.,  6.,  8., 10.]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 在1维度连接张量\n",
    "D = torch.cat((A,B),dim=1)\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  0.,  1.,  2.,  0.,  2.,  4.],\n",
       "        [ 4.,  5.,  3.,  4.,  5.,  6.,  8., 10.]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 在1维度连接3个张量\n",
    "E = torch.cat((A[:,1:3],A,B),dim=1)## A[:,1:3]表示A张量0维全部，1维元素1~2\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.],\n",
      "         [ 0.,  2.,  4.]],\n",
      "\n",
      "        [[ 3.,  4.,  5.],\n",
      "         [ 6.,  8., 10.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 沿着新维度连接张量\n",
    "F = torch.stack((A,B),dim=1)\n",
    "print(F)\n",
    "F.shape ## 两个2x3的矩阵组合在一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1, 2],\n",
      "          [1, 3]],\n",
      "\n",
      "         [[3, 4],\n",
      "          [3, 5]]],\n",
      "\n",
      "\n",
      "        [[[5, 6],\n",
      "          [5, 7]],\n",
      "\n",
      "         [[7, 8],\n",
      "          [7, 9]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 2, 2])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = torch.stack((A,B),dim=2)\n",
    "print(G)\n",
    "G.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 0., 1., 2., 0., 2., 4.]]),\n",
       " tensor([[ 4.,  5.,  3.,  4.,  5.,  6.,  8., 10.]]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 将张量分割为特定数量的块\n",
    "## 在行上将张量E分为两块\n",
    "torch.chunk(E,2,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 0., 1.],\n",
      "        [4., 5., 3., 4.]])\n",
      "tensor([[ 2.,  0.,  2.,  4.],\n",
      "        [ 5.,  6.,  8., 10.]])\n"
     ]
    }
   ],
   "source": [
    "D1,D2=torch.chunk(E,2,dim=1)\n",
    "print(D1)\n",
    "print(D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 0.],\n",
      "        [4., 5., 3.]])\n",
      "tensor([[1., 2., 0.],\n",
      "        [4., 5., 6.]])\n",
      "tensor([[ 2.,  4.],\n",
      "        [ 8., 10.]])\n"
     ]
    }
   ],
   "source": [
    "## 如果沿给定维度dim的张量大小不能被整除，则最后一个块将最小\n",
    "E1,E2,E3=torch.chunk(E,3,dim=1)\n",
    "print(E1)\n",
    "print(E2)\n",
    "print(E3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [3.]])\n",
      "tensor([[1., 2.],\n",
      "        [4., 5.]])\n",
      "tensor([[ 0.,  2.,  4.],\n",
      "        [ 6.,  8., 10.]])\n"
     ]
    }
   ],
   "source": [
    "## 将张量切分为块，指定每块的大小\n",
    "D1,D2,D3=torch.split(D,[1,2,3],dim=1)\n",
    "print(D1)\n",
    "print(D2)\n",
    "print(D3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4张量计算 比较大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## 比较两个数是否接近\n",
    "A = torch.tensor([10.0])\n",
    "B = torch.tensor([10.1])\n",
    "print(torch.allclose(A,B,rtol=1e-05,atol=1e-08,equal_nan=False))\n",
    "print(torch.allclose(A,B,rtol=0.1,atol=0.01,equal_nan=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "tensor([True, True, True, True, True, True])\n",
      "tensor([[True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "## 计算元素是否相等\n",
    "A = torch.tensor([1,2,3,4,5,6])\n",
    "B = torch.arange(1,7)\n",
    "C = torch.unsqueeze(B,dim=0)\n",
    "print(C)\n",
    "# torch.eq()是比较内部的元素\n",
    "print(torch.eq(A,B))\n",
    "print(torch.eq(A,C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "## 判断两个张量是否具有相同的尺寸和元素\n",
    "print(torch.equal(A,B))\n",
    "print(torch.equal(A,C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True])\n",
      "tensor([[True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "## 逐元素比较大于等于\n",
    "print(torch.ge(A,B))\n",
    "print(torch.ge(A,C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False, False, False, False, False])\n",
      "tensor([[False, False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "## 大于\n",
    "print(torch.gt(A,B))\n",
    "print(torch.gt(A,C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True])\n",
      "tensor([[False, False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "## 小于等于/小于\n",
    "print(torch.le(A,B))\n",
    "print(torch.lt(A,C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False, False, False, False, False])\n",
      "tensor([[False, False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "## 不等于\n",
    "print(torch.ne(A,B))\n",
    "print(torch.ne(A,C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False,  True, False])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 判断是否为缺失值\n",
    "torch.isnan(torch.tensor([0,1,float(\"nan\"),2]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5基本运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n",
      "B: tensor([[10., 12., 14.],\n",
      "        [16., 18., 20.]])\n",
      "tensor([[  0.,  12.,  28.],\n",
      "        [ 48.,  72., 100.]])\n",
      "tensor([[0.0000, 0.0833, 0.1429],\n",
      "        [0.1875, 0.2222, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "## 矩阵逐元素相乘\n",
    "A = torch.arange(6.0).reshape(2,3)\n",
    "B = torch.linspace(10,20,6).reshape(2,3)\n",
    "print(\"A:\",A)\n",
    "print(\"B:\",B)\n",
    "print(A*B)\n",
    "##逐元素相除\n",
    "print(A/B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10., 13., 16.],\n",
      "        [19., 22., 25.]])\n",
      "tensor([[-10., -11., -12.],\n",
      "        [-13., -14., -15.]])\n",
      "tensor([[    inf, 12.0000,  7.0000],\n",
      "        [ 5.3333,  4.5000,  4.0000]])\n"
     ]
    }
   ],
   "source": [
    "## 逐元素相加\n",
    "print(A+B)\n",
    "## 逐元素相减\n",
    "print(A-B)\n",
    "## 逐元素相除\n",
    "print(B/A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.,   1.,   8.],\n",
      "        [ 27.,  64., 125.]])\n",
      "tensor([[  0.,   1.,   8.],\n",
      "        [ 27.,  64., 125.]])\n"
     ]
    }
   ],
   "source": [
    "## 张量的幂\n",
    "print(torch.pow(A,3))\n",
    "print(A**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1.0000,   2.7183,   7.3891],\n",
       "        [ 20.0855,  54.5981, 148.4132]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 张量的指数\n",
    "torch.exp(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -inf, 0.0000, 0.6931],\n",
       "        [1.0986, 1.3863, 1.6094]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 张量的对数\n",
    "torch.log(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 1.0000, 1.4142],\n",
      "        [1.7321, 2.0000, 2.2361]])\n",
      "tensor([[0.0000, 1.0000, 1.4142],\n",
      "        [1.7321, 2.0000, 2.2361]])\n"
     ]
    }
   ],
   "source": [
    "## 张量的平方根\n",
    "print(torch.sqrt(A))\n",
    "print(A**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   inf, 1.0000, 0.7071],\n",
      "        [0.5774, 0.5000, 0.4472]])\n",
      "tensor([[   inf, 1.0000, 0.7071],\n",
      "        [0.5774, 0.5000, 0.4472]])\n"
     ]
    }
   ],
   "source": [
    "## 张量的平方根倒数\n",
    "print(torch.rsqrt(A))\n",
    "print(1/A**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "796243f116c04d1ce56ed7e8d53e4c376a378c20bd451e5996b1ae453cf668c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
